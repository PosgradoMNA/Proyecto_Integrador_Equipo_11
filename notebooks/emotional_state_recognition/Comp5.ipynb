{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"},"colab":{"name":"Comp5.ipynb","provenance":[{"file_id":"1SOW-Ustwt6bHQYLuiqmC1NopTAX-y3cj","timestamp":1604701738537}],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"id":"DLCtgB5RkXzZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620808729770,"user_tz":300,"elapsed":3045442,"user":{"displayName":"Oliver Velázquez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggcenj_Ljvsw37meOz8bjetFfl1PkEzd4uTuqtZ=s64","userId":"12153848691637648939"}},"outputId":"df3a06ac-7eee-4638-ae3f-940ad3d6b3fc"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zu4csy1oehXN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620808735812,"user_tz":300,"elapsed":436,"user":{"displayName":"Oliver Velázquez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggcenj_Ljvsw37meOz8bjetFfl1PkEzd4uTuqtZ=s64","userId":"12153848691637648939"}},"outputId":"7dae1140-2268-4633-b373-69038a884414"},"source":["# !ls /content/drive/My\\ Drive/JnOTE/graficos/1esp_15hz_1hz_1/\n","# !ls /content/drive/My\\ Drive/JnOTE/graficos/\n","!ls /content/drive/My\\ Drive/JnOTE/graficos/oth_ith_4/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["dep_writing_Total_25_oth0.07_ith0.5.csv\n","dep_writing_Total_25_oth0.09_ith0.5.csv\n","dep_writing_Total_25_oth0.11_ith0.5.csv\n","dep_writing_Total_25_oth0.13_ith0.5.csv\n","dep_writing_Total_25_oth0.15_ith0.5.csv\n","dep_writing_Total_25_oth0.17_ith0.5.csv\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"laMjojuY5LxU"},"source":["for name in dir():\n","       if not name.startswith('_'):\n","           del globals()[name]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NO35rHwzG87b"},"source":["nn = 60\n","mm = 5\n","method = \"OTH\"\n","# pat = \"/content/drive/My Drive/JnOTE/graficos/\"\n","pat = \"/content/drive/My Drive/JnOTE/graficos/oth_ith_4/\"\n","# pat = \"/content/drive/My Drive/JnOTE/graficos/1esp_15hz_1hz_1/\"\n","fileN = \"dep_writing_Total_25_oth0.15_ith0.5.csv\"\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aYFZ6CAO5OaI"},"source":["# Reset Variables"]},{"cell_type":"code","metadata":{"id":"v3A8fMJ9kUg3"},"source":["# Importar librerías\n","import pandas as pd\n","import numpy as np\n","import math\n","import statistics as sta\n","import matplotlib.pyplot as plt\n","from sklearn.decomposition import PCA\n","from sklearn.decomposition import FastICA\n","from sklearn.manifold import Isomap\n","from sklearn.model_selection import cross_val_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","from sklearn.svm import LinearSVC\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.linear_model import LogisticRegressionCV\n","from sklearn.naive_bayes import MultinomialNB \n","from sklearn.ensemble import VotingClassifier\n","from random import seed\n","from random import randint\n","import random\n","\n","from sklearn.metrics import roc_auc_score\n","from sklearn import metrics       \n","from sklearn.metrics import classification_report, confusion_matrix\n","from sklearn.metrics import roc_curve           \n","\n","from sklearn import metrics \n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import roc_curve                         # thresholds = all unique prediction probabilities in descending order\n","\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n","from sklearn.manifold import LocallyLinearEmbedding\n","\n","import time"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7hXX0c6D8cNH"},"source":["def agument_Data(x, Eps, proportion):\n","  N=int(proportion*len(x))\n","  df_to_be_agumented=x\n","  rn=rng.normal(0, Eps, size=(1, x.shape[1]))\n","  #print('rn=',rn)\n","\n","  for i in range(N):                            # create new N data. Where N are the required proportion.\n","    rd_row=random.randint(0,len(x)-1)           # randomly select one row to include noise.\n","    #print('i',i,'rd_row',rd_row)\n","    b=x.values[rd_row,0:x.shape[1]]+rn          # add Gaussian Noise to the selected observation.\n","    b=pd.DataFrame(data=b)                      # return format to Panda Dataframe\n","    b.columns = x.columns                       # re-name the new dataframe with the original names of the columns\n","    b.values[0,b.shape[1]-1]=int(x.values[rd_row,b.shape[1]-1]) # convert to integer the last value (this is the value used to classify)\n","    df_to_be_agumented = pd.concat([df_to_be_agumented,b])      # concat the augmente observation at the end of the array.\n","    df_to_be_agumented.reset_index(inplace=True, drop=True)     # reset the index\n","  return(df_to_be_agumented)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DeZ78OA5twrn"},"source":["\n","path = pat\n","file= path + fileN\n","data = pd.read_csv(file, header = None)\n","data_name = []\n","for i in range(len(data.axes[1])):\n","\n","  if i == len(data.axes[1])-1:\n","    data_name = data_name + ['Emotions']\n","  else:\n","    data_name = data_name + [str(i)]\n","\n","data_n = data_name\n","data.columns = data_name\n","def make_dataframes_classes_same_length(x, Eps, prop):\n","  e0=x[(x['Emotions']==0)]\n","  e1=x[(x['Emotions']==1)]\n","  l_e0=len(e0)\n","  l_e1=len(e1)\n","  #print('e0',len(e0))\n","  #print('e1',len(e1))\n","  if l_e0>l_e1:\n","      prop=l_e0/l_e1-1\n","      e1=agument_Data(e1, Eps, prop)\n","  else:\n","      prop=e1/l_e0-1\n","      e0=agument_Data(e0,prop, Eps)\n","  df_to_be_agumented = pd.concat([e0,e1])                   # concat the augmente observation at the end of the array.\n","  df_to_be_agumented.reset_index(inplace=True, drop=True)   # reset the index\n","  return df_to_be_agumented \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7b4JBFZL4yEt"},"source":["e0=data[(data['Emotions']==0)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GbxjCEaryfq2"},"source":["####################################################################################\n","# suffle_df: retruns the dataframes with segments i and j of length k shuffled.\n","#\n","# Input:\n","#   X is the input matrix to be shuffled.\n","#   i is the starting rows of the first segment of rows to shuffle\n","#   j is the starting rows of the second segment of rows to shuffle\n","#   k is the number of rows to shuffle\n","#  Output:\n","#   the dataframes with segments i and j of length k shuffled.\n","####################################################################################\n","def suffle_df(X, i, j, k):\n","  #a=X.iloc[0:8].copy()\n","  a=X\n","  #print(a)\n","  b,c=a.iloc[0:i+k].copy(),a.iloc[j:j+k].copy()\n","  b.reset_index(inplace=True, drop=True)\n","  c.reset_index(inplace=True, drop=True)\n","  b.index=b.index + j  # modify the values of the index such that 'b' variable can be replaced.\n","  #print(b)\n","  #print(c)\n","  a.iloc[0:i+k], a.iloc[j:j+k]=c,b\n","  return(a)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HdA_NqngfH2Y"},"source":["# #PCA\n","# def PComp(dataset,n):\n","#   fDum = dataset.columns\n","#   features = fDum[0:-1] \n","#   x = dataset.loc[:,features].values\n","#   y = dataset.loc[:,[\"Emotions\"]].values\n","#   pca = PCA(n_components=n)\n","#   prinComp = pca.fit_transform(x)\n","#   names = []\n","#   for i in range(n):\n","#       names = names + [\"P Component\" + str(i+1)]\n","#   prinDf = pd.DataFrame(data = prinComp, columns = names)\n","#   finalDf = pd.concat([prinDf,dataset[[\"Emotions\"]]], axis = 1)\n","#   return finalDf,pca\n","# data,pca = PComp(data, 20)\n","# data\n","#LLE\n","def comp(dataset,datas, n,m):\n","  # fDum = dataset.columns\n","  # features = fDum[0:-1] \n","  # print(features)\n","  # x = dataset.loc[:,features].values\n","  # y = dataset.loc[:,[\"Emotions\"]].values\n","  if method == \"LLE\":\n","\n","    embedding = LocallyLinearEmbedding(n_components=n,n_neighbors=m)\n","    X_trans = embedding.fit_transform(dataset)\n","    X_test = embedding.transform(datas)\n","  elif method == \"PCA\":\n","    \n","    pca = PCA(n_components=n)\n","    X_trans = pca.fit_transform(dataset)\n","    X_test = pca.transform(datas)\n","  elif method == \"ICA\":\n","    ica = FastICA(n_components = n, random_state = 0, max_iter = 250, tol = 0.1)\n","    X_trans = ica.fit_transform(dataset)\n","    X_test = ica.transform(datas)\n","  elif method == \"ISO\":\n","    iso = Isomap(n_components = n, n_neighbors = m)\n","    X_trans = iso.fit_transform(dataset)\n","    X_test = iso.transform(datas)\n","  else:\n","    quit()\n","  names = []\n","  for i in range(n):\n","    names = names + [\"Component \" + str(i+1)]\n","  prinDf = pd.DataFrame(data = X_trans, columns = names)\n","  prinDf2 = pd.DataFrame(data = X_test, columns = None)\n","  # finalDf = pd.concat([prinDf,dataset[[\"Emotions\"]]], axis = 1)\n","  # return finalDf\n","  return prinDf, prinDf2\n","# data_new = comp(data, nn,mm)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"slhrX1Ff9DJS"},"source":["####################################################################################\n","####################################################################################\n","####################################################################################\n","def train_test_sets(x, i, k, Eps, pr,n,m):\n","  X_test2=x.values[i:k,col]\n","  y_test=x.values[i:k,nf]\n","  tmp=x.loc[k:len(x)]\n","  #print('Input tmp.shape[0]',tmp.shape[0])\n","  tmp = make_dataframes_classes_same_length(tmp, Eps, pr)\n","  # print(tmp2.shape)\n","  # print(X_test2.shape)\n","  \n","  # X_test = LLEcomp(X_test,n,7)\n","  #tmp = agument_Data(tmp, Eps, pr)\n","  #print('Output tmp.shape[0]', tmp.shape[0])\n","  X_train2=tmp.values[k:len(tmp),col]\n","  y_train=tmp.values[k:len(tmp),nf]\n","  # X_train2=x.values[k:len(x),col]\n","  # y_train=x.values[k:len(x),nf]\n","  # X_train2=tmp.values[k:len(tmp),range(0,n+1)]\n","  # y_train=tmp.values[k:len(tmp),n]\n","  # print(X_train2.shape)\n","  # print(X_test2.shape)\n","  if \"oth\" in fileN:\n","    X_train, X_test = X_train2, X_test2\n","  else:\n","    X_train, X_test = comp(X_train2,X_test2,nn,m)\n","\n","  # print(X_train.shape)\n","  # print(y_train.shape)\n","  # print(X_test.shape)\n","  # print(y_test.shape)\n","  #print('X_train.shape[0]', X_train.shape[0])\n","  #print('y_train.shape[0]', y_train.shape[0])\n","\n","  return X_train, X_test, y_train, y_test\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8a4pj56ac-m4","colab":{"base_uri":"https://localhost:8080/","height":439},"executionInfo":{"status":"ok","timestamp":1620808766296,"user_tz":300,"elapsed":576,"user":{"displayName":"Oliver Velázquez","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggcenj_Ljvsw37meOz8bjetFfl1PkEzd4uTuqtZ=s64","userId":"12153848691637648939"}},"outputId":"60361fcb-6c75-421f-b502-0336c74fcd7d"},"source":["def normalize(dataset):\n","    dataNorm=(dataset-dataset.mean())/(dataset.std())\n","    dataNorm[\"Emotions\"]=dataset[\"Emotions\"]\n","    return dataNorm\n","data=normalize(data)\n","data"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>10</th>\n","      <th>11</th>\n","      <th>12</th>\n","      <th>13</th>\n","      <th>14</th>\n","      <th>15</th>\n","      <th>16</th>\n","      <th>17</th>\n","      <th>18</th>\n","      <th>19</th>\n","      <th>20</th>\n","      <th>21</th>\n","      <th>22</th>\n","      <th>23</th>\n","      <th>24</th>\n","      <th>25</th>\n","      <th>26</th>\n","      <th>27</th>\n","      <th>28</th>\n","      <th>29</th>\n","      <th>30</th>\n","      <th>31</th>\n","      <th>32</th>\n","      <th>33</th>\n","      <th>34</th>\n","      <th>35</th>\n","      <th>36</th>\n","      <th>37</th>\n","      <th>38</th>\n","      <th>39</th>\n","      <th>40</th>\n","      <th>41</th>\n","      <th>42</th>\n","      <th>43</th>\n","      <th>44</th>\n","      <th>45</th>\n","      <th>46</th>\n","      <th>47</th>\n","      <th>48</th>\n","      <th>49</th>\n","      <th>50</th>\n","      <th>51</th>\n","      <th>52</th>\n","      <th>53</th>\n","      <th>54</th>\n","      <th>55</th>\n","      <th>56</th>\n","      <th>57</th>\n","      <th>58</th>\n","      <th>59</th>\n","      <th>60</th>\n","      <th>61</th>\n","      <th>62</th>\n","      <th>63</th>\n","      <th>64</th>\n","      <th>65</th>\n","      <th>66</th>\n","      <th>67</th>\n","      <th>68</th>\n","      <th>69</th>\n","      <th>70</th>\n","      <th>71</th>\n","      <th>Emotions</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-0.347844</td>\n","      <td>2.189228</td>\n","      <td>0.463538</td>\n","      <td>-1.801627</td>\n","      <td>0.632033</td>\n","      <td>-0.553422</td>\n","      <td>-0.455269</td>\n","      <td>-0.366360</td>\n","      <td>-0.462043</td>\n","      <td>0.536388</td>\n","      <td>-0.706399</td>\n","      <td>-2.195129</td>\n","      <td>-1.222446</td>\n","      <td>-2.212059</td>\n","      <td>-0.226695</td>\n","      <td>0.774891</td>\n","      <td>0.055841</td>\n","      <td>0.632462</td>\n","      <td>-1.079671</td>\n","      <td>-1.090790</td>\n","      <td>-1.017814</td>\n","      <td>-2.230147</td>\n","      <td>-2.489456</td>\n","      <td>-0.907112</td>\n","      <td>0.748609</td>\n","      <td>4.267456</td>\n","      <td>1.083494</td>\n","      <td>1.487158</td>\n","      <td>-0.635287</td>\n","      <td>0.161437</td>\n","      <td>-0.497632</td>\n","      <td>-0.343546</td>\n","      <td>0.132867</td>\n","      <td>0.120168</td>\n","      <td>-0.153347</td>\n","      <td>0.017513</td>\n","      <td>0.533959</td>\n","      <td>-0.59592</td>\n","      <td>-0.454652</td>\n","      <td>0.042539</td>\n","      <td>-2.175388</td>\n","      <td>0.194210</td>\n","      <td>1.426720</td>\n","      <td>1.897496</td>\n","      <td>-0.024237</td>\n","      <td>1.893892</td>\n","      <td>-0.983550</td>\n","      <td>-0.591281</td>\n","      <td>-0.459416</td>\n","      <td>0.637440</td>\n","      <td>-2.203682</td>\n","      <td>-1.335505</td>\n","      <td>-0.433110</td>\n","      <td>-0.232504</td>\n","      <td>-1.137321</td>\n","      <td>-0.398924</td>\n","      <td>-0.808538</td>\n","      <td>0.697608</td>\n","      <td>0.280723</td>\n","      <td>-1.106731</td>\n","      <td>-0.722058</td>\n","      <td>-0.137730</td>\n","      <td>-0.627346</td>\n","      <td>0.216590</td>\n","      <td>1.442091</td>\n","      <td>-0.263486</td>\n","      <td>2.551502</td>\n","      <td>-0.164256</td>\n","      <td>1.957830</td>\n","      <td>0.805461</td>\n","      <td>-1.241912</td>\n","      <td>-0.154762</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-0.503110</td>\n","      <td>0.280254</td>\n","      <td>-1.347963</td>\n","      <td>-0.058510</td>\n","      <td>-0.566242</td>\n","      <td>-0.124410</td>\n","      <td>-0.556104</td>\n","      <td>-0.290405</td>\n","      <td>-0.321090</td>\n","      <td>2.001509</td>\n","      <td>0.461881</td>\n","      <td>-0.304357</td>\n","      <td>1.182715</td>\n","      <td>1.225488</td>\n","      <td>-0.868779</td>\n","      <td>-1.433517</td>\n","      <td>-0.401772</td>\n","      <td>0.592280</td>\n","      <td>-0.022377</td>\n","      <td>-0.369429</td>\n","      <td>0.821072</td>\n","      <td>-0.483921</td>\n","      <td>0.441827</td>\n","      <td>-0.085772</td>\n","      <td>-0.716672</td>\n","      <td>-0.416869</td>\n","      <td>0.959260</td>\n","      <td>-0.695879</td>\n","      <td>-0.303459</td>\n","      <td>0.853909</td>\n","      <td>1.068091</td>\n","      <td>-1.034253</td>\n","      <td>-0.731130</td>\n","      <td>-0.430382</td>\n","      <td>0.102911</td>\n","      <td>-0.251604</td>\n","      <td>-0.145263</td>\n","      <td>-0.59592</td>\n","      <td>-0.883968</td>\n","      <td>-0.518293</td>\n","      <td>-0.802016</td>\n","      <td>-0.482656</td>\n","      <td>-0.659171</td>\n","      <td>1.148316</td>\n","      <td>0.044111</td>\n","      <td>-0.610708</td>\n","      <td>0.792701</td>\n","      <td>1.475319</td>\n","      <td>0.410914</td>\n","      <td>-0.799359</td>\n","      <td>-1.495614</td>\n","      <td>-0.093415</td>\n","      <td>-0.130537</td>\n","      <td>0.094612</td>\n","      <td>0.367879</td>\n","      <td>0.706725</td>\n","      <td>0.380194</td>\n","      <td>-0.317259</td>\n","      <td>0.147259</td>\n","      <td>0.029110</td>\n","      <td>0.132494</td>\n","      <td>-0.832425</td>\n","      <td>-0.475283</td>\n","      <td>-0.522627</td>\n","      <td>-0.273986</td>\n","      <td>-0.016274</td>\n","      <td>0.227323</td>\n","      <td>-1.157530</td>\n","      <td>-1.188340</td>\n","      <td>-1.287310</td>\n","      <td>-0.806115</td>\n","      <td>-0.198937</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-0.037312</td>\n","      <td>0.484960</td>\n","      <td>0.963926</td>\n","      <td>-0.462755</td>\n","      <td>0.156687</td>\n","      <td>-0.002571</td>\n","      <td>-0.005037</td>\n","      <td>-0.349454</td>\n","      <td>-0.634015</td>\n","      <td>1.081400</td>\n","      <td>-1.588844</td>\n","      <td>-0.050649</td>\n","      <td>-1.896508</td>\n","      <td>-2.091472</td>\n","      <td>0.692687</td>\n","      <td>1.220831</td>\n","      <td>-1.023721</td>\n","      <td>-0.123929</td>\n","      <td>-0.764112</td>\n","      <td>-1.799578</td>\n","      <td>-0.505701</td>\n","      <td>2.111437</td>\n","      <td>-0.884590</td>\n","      <td>-0.700450</td>\n","      <td>-0.296839</td>\n","      <td>-0.300457</td>\n","      <td>0.859547</td>\n","      <td>-1.455747</td>\n","      <td>-1.174632</td>\n","      <td>1.614772</td>\n","      <td>-0.166877</td>\n","      <td>0.315606</td>\n","      <td>0.187450</td>\n","      <td>-0.706769</td>\n","      <td>-0.236035</td>\n","      <td>-0.191137</td>\n","      <td>-0.704568</td>\n","      <td>-0.59592</td>\n","      <td>0.206790</td>\n","      <td>0.893521</td>\n","      <td>-0.448994</td>\n","      <td>-0.507871</td>\n","      <td>-0.030570</td>\n","      <td>0.640251</td>\n","      <td>0.007614</td>\n","      <td>-0.363689</td>\n","      <td>-0.232789</td>\n","      <td>-1.540276</td>\n","      <td>-1.766800</td>\n","      <td>0.982071</td>\n","      <td>1.018257</td>\n","      <td>0.461155</td>\n","      <td>-0.008208</td>\n","      <td>0.890140</td>\n","      <td>0.728218</td>\n","      <td>0.603371</td>\n","      <td>0.094229</td>\n","      <td>0.303877</td>\n","      <td>-0.161572</td>\n","      <td>-0.376768</td>\n","      <td>-0.657849</td>\n","      <td>0.198502</td>\n","      <td>-0.172130</td>\n","      <td>-0.108749</td>\n","      <td>0.803189</td>\n","      <td>0.472073</td>\n","      <td>-0.475788</td>\n","      <td>-0.799827</td>\n","      <td>0.972928</td>\n","      <td>1.705026</td>\n","      <td>-0.336069</td>\n","      <td>-0.500178</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>-0.192578</td>\n","      <td>-0.145812</td>\n","      <td>2.744152</td>\n","      <td>1.611851</td>\n","      <td>-1.012134</td>\n","      <td>-0.474206</td>\n","      <td>1.844317</td>\n","      <td>-0.797957</td>\n","      <td>-0.766887</td>\n","      <td>0.477078</td>\n","      <td>-1.054581</td>\n","      <td>-0.915834</td>\n","      <td>-0.869565</td>\n","      <td>0.419974</td>\n","      <td>1.759775</td>\n","      <td>0.032289</td>\n","      <td>-0.276572</td>\n","      <td>2.652707</td>\n","      <td>0.601547</td>\n","      <td>-0.924537</td>\n","      <td>-1.733231</td>\n","      <td>0.042723</td>\n","      <td>0.265512</td>\n","      <td>-1.649626</td>\n","      <td>0.172891</td>\n","      <td>0.616791</td>\n","      <td>0.084723</td>\n","      <td>0.608873</td>\n","      <td>-0.619905</td>\n","      <td>-0.441269</td>\n","      <td>-0.162459</td>\n","      <td>0.655700</td>\n","      <td>0.269832</td>\n","      <td>-0.444227</td>\n","      <td>0.992210</td>\n","      <td>0.008807</td>\n","      <td>-0.167134</td>\n","      <td>1.66507</td>\n","      <td>-0.498751</td>\n","      <td>-1.260515</td>\n","      <td>-0.885717</td>\n","      <td>0.370143</td>\n","      <td>-1.705422</td>\n","      <td>1.353264</td>\n","      <td>0.119203</td>\n","      <td>0.415127</td>\n","      <td>-0.687849</td>\n","      <td>0.524627</td>\n","      <td>-0.963591</td>\n","      <td>-1.167562</td>\n","      <td>0.953530</td>\n","      <td>-0.296722</td>\n","      <td>-0.563166</td>\n","      <td>-1.309302</td>\n","      <td>-0.336701</td>\n","      <td>-1.716442</td>\n","      <td>-1.459843</td>\n","      <td>-1.041672</td>\n","      <td>3.177562</td>\n","      <td>-0.841310</td>\n","      <td>-0.260745</td>\n","      <td>2.400550</td>\n","      <td>0.449316</td>\n","      <td>-0.648443</td>\n","      <td>-0.433712</td>\n","      <td>-0.312118</td>\n","      <td>-0.504842</td>\n","      <td>0.704725</td>\n","      <td>1.765555</td>\n","      <td>1.243591</td>\n","      <td>0.667798</td>\n","      <td>0.917145</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-0.347844</td>\n","      <td>-1.565319</td>\n","      <td>-0.585353</td>\n","      <td>-0.644989</td>\n","      <td>0.578867</td>\n","      <td>-0.231275</td>\n","      <td>-1.853165</td>\n","      <td>-0.391142</td>\n","      <td>-0.696378</td>\n","      <td>1.020487</td>\n","      <td>-0.422071</td>\n","      <td>0.500134</td>\n","      <td>-0.630630</td>\n","      <td>-0.530759</td>\n","      <td>0.134701</td>\n","      <td>-0.306523</td>\n","      <td>-0.299102</td>\n","      <td>-0.653630</td>\n","      <td>0.525456</td>\n","      <td>0.155408</td>\n","      <td>-1.387807</td>\n","      <td>0.366157</td>\n","      <td>0.811176</td>\n","      <td>0.756806</td>\n","      <td>-0.630264</td>\n","      <td>-0.681392</td>\n","      <td>0.689543</td>\n","      <td>-0.360309</td>\n","      <td>-0.534427</td>\n","      <td>0.980719</td>\n","      <td>-0.636051</td>\n","      <td>-0.753763</td>\n","      <td>0.253898</td>\n","      <td>0.172919</td>\n","      <td>0.150632</td>\n","      <td>-0.035486</td>\n","      <td>-0.230691</td>\n","      <td>-0.59592</td>\n","      <td>0.350717</td>\n","      <td>0.748955</td>\n","      <td>-0.175686</td>\n","      <td>-0.591145</td>\n","      <td>-0.642877</td>\n","      <td>-0.498159</td>\n","      <td>0.450074</td>\n","      <td>-0.356621</td>\n","      <td>0.000505</td>\n","      <td>0.238329</td>\n","      <td>-1.354130</td>\n","      <td>-0.253930</td>\n","      <td>0.216695</td>\n","      <td>0.214909</td>\n","      <td>-0.785781</td>\n","      <td>-0.659238</td>\n","      <td>-0.743665</td>\n","      <td>-0.471888</td>\n","      <td>-1.019763</td>\n","      <td>0.165956</td>\n","      <td>-0.740047</td>\n","      <td>-0.289926</td>\n","      <td>0.230568</td>\n","      <td>-1.374269</td>\n","      <td>1.322001</td>\n","      <td>-0.125273</td>\n","      <td>-1.225522</td>\n","      <td>0.139754</td>\n","      <td>-0.446733</td>\n","      <td>-1.151674</td>\n","      <td>1.669012</td>\n","      <td>0.649319</td>\n","      <td>-1.628106</td>\n","      <td>0.197839</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>124</th>\n","      <td>-0.037312</td>\n","      <td>0.740708</td>\n","      <td>0.579012</td>\n","      <td>-0.604530</td>\n","      <td>-0.793449</td>\n","      <td>-0.241402</td>\n","      <td>1.478369</td>\n","      <td>0.508773</td>\n","      <td>-0.425941</td>\n","      <td>1.135901</td>\n","      <td>-0.601985</td>\n","      <td>0.420064</td>\n","      <td>-0.297451</td>\n","      <td>1.534813</td>\n","      <td>-0.792172</td>\n","      <td>1.624082</td>\n","      <td>-1.216978</td>\n","      <td>1.130321</td>\n","      <td>-0.159418</td>\n","      <td>0.463233</td>\n","      <td>-0.894001</td>\n","      <td>-0.882770</td>\n","      <td>2.715910</td>\n","      <td>0.646940</td>\n","      <td>1.101184</td>\n","      <td>0.068700</td>\n","      <td>-0.498847</td>\n","      <td>-0.080612</td>\n","      <td>0.262361</td>\n","      <td>0.913752</td>\n","      <td>1.024114</td>\n","      <td>1.076435</td>\n","      <td>0.128630</td>\n","      <td>-0.882504</td>\n","      <td>-0.813501</td>\n","      <td>0.806839</td>\n","      <td>0.770173</td>\n","      <td>1.66507</td>\n","      <td>-0.183157</td>\n","      <td>-0.150356</td>\n","      <td>0.975054</td>\n","      <td>-0.619607</td>\n","      <td>-0.900740</td>\n","      <td>0.853811</td>\n","      <td>-0.073174</td>\n","      <td>0.023208</td>\n","      <td>-0.285593</td>\n","      <td>1.207782</td>\n","      <td>2.353812</td>\n","      <td>-0.289069</td>\n","      <td>-0.392976</td>\n","      <td>0.716712</td>\n","      <td>0.846687</td>\n","      <td>-0.119670</td>\n","      <td>-0.016564</td>\n","      <td>-0.161252</td>\n","      <td>1.050941</td>\n","      <td>-0.795713</td>\n","      <td>0.491783</td>\n","      <td>1.212235</td>\n","      <td>-0.636454</td>\n","      <td>2.178696</td>\n","      <td>-0.002547</td>\n","      <td>-0.410343</td>\n","      <td>0.726247</td>\n","      <td>0.512600</td>\n","      <td>0.901074</td>\n","      <td>0.910287</td>\n","      <td>1.084885</td>\n","      <td>0.292755</td>\n","      <td>0.579222</td>\n","      <td>-1.008988</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>125</th>\n","      <td>-0.968907</td>\n","      <td>-1.075315</td>\n","      <td>-0.279827</td>\n","      <td>-0.266242</td>\n","      <td>-1.239804</td>\n","      <td>-1.082217</td>\n","      <td>0.436001</td>\n","      <td>-0.183183</td>\n","      <td>-0.397725</td>\n","      <td>1.038120</td>\n","      <td>-0.536624</td>\n","      <td>-1.009445</td>\n","      <td>-1.329615</td>\n","      <td>1.242363</td>\n","      <td>-1.253936</td>\n","      <td>0.254516</td>\n","      <td>-0.768252</td>\n","      <td>-0.866673</td>\n","      <td>1.130868</td>\n","      <td>-0.192000</td>\n","      <td>0.060851</td>\n","      <td>0.062287</td>\n","      <td>0.539519</td>\n","      <td>0.505625</td>\n","      <td>-0.512930</td>\n","      <td>0.172560</td>\n","      <td>0.439442</td>\n","      <td>-0.256403</td>\n","      <td>-1.316087</td>\n","      <td>0.682928</td>\n","      <td>1.647090</td>\n","      <td>0.648687</td>\n","      <td>-0.072919</td>\n","      <td>-0.159755</td>\n","      <td>-0.934113</td>\n","      <td>1.627171</td>\n","      <td>-0.450093</td>\n","      <td>1.66507</td>\n","      <td>-0.848935</td>\n","      <td>1.247125</td>\n","      <td>-0.116469</td>\n","      <td>-1.060852</td>\n","      <td>-2.436759</td>\n","      <td>-0.620439</td>\n","      <td>0.147606</td>\n","      <td>0.307428</td>\n","      <td>-0.877078</td>\n","      <td>1.765044</td>\n","      <td>0.790145</td>\n","      <td>-0.302820</td>\n","      <td>0.404009</td>\n","      <td>-0.292584</td>\n","      <td>2.226771</td>\n","      <td>1.152199</td>\n","      <td>0.541627</td>\n","      <td>0.679059</td>\n","      <td>-0.517988</td>\n","      <td>-0.213490</td>\n","      <td>-0.672345</td>\n","      <td>-1.004179</td>\n","      <td>-0.378351</td>\n","      <td>0.004974</td>\n","      <td>1.476552</td>\n","      <td>-0.762029</td>\n","      <td>-1.249871</td>\n","      <td>0.229926</td>\n","      <td>-0.542153</td>\n","      <td>0.120186</td>\n","      <td>-1.512043</td>\n","      <td>-1.343242</td>\n","      <td>-0.887606</td>\n","      <td>-0.295270</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>126</th>\n","      <td>-0.037312</td>\n","      <td>-1.216084</td>\n","      <td>0.600663</td>\n","      <td>0.272299</td>\n","      <td>-0.296614</td>\n","      <td>-0.219097</td>\n","      <td>-0.226322</td>\n","      <td>-0.425356</td>\n","      <td>-0.860126</td>\n","      <td>0.517152</td>\n","      <td>0.707294</td>\n","      <td>0.746440</td>\n","      <td>-0.192165</td>\n","      <td>0.797108</td>\n","      <td>0.822989</td>\n","      <td>-0.978459</td>\n","      <td>-0.114809</td>\n","      <td>0.316057</td>\n","      <td>-0.688297</td>\n","      <td>1.371337</td>\n","      <td>1.737865</td>\n","      <td>0.890277</td>\n","      <td>0.570603</td>\n","      <td>0.073921</td>\n","      <td>-0.194109</td>\n","      <td>-0.142840</td>\n","      <td>0.833392</td>\n","      <td>-0.066269</td>\n","      <td>-0.109810</td>\n","      <td>0.662980</td>\n","      <td>-1.192707</td>\n","      <td>1.398998</td>\n","      <td>-0.691804</td>\n","      <td>2.078118</td>\n","      <td>-0.222859</td>\n","      <td>-0.893925</td>\n","      <td>0.275359</td>\n","      <td>1.66507</td>\n","      <td>-0.043961</td>\n","      <td>-0.344417</td>\n","      <td>-0.468922</td>\n","      <td>-0.300072</td>\n","      <td>-0.344240</td>\n","      <td>-0.334544</td>\n","      <td>1.975002</td>\n","      <td>-0.339322</td>\n","      <td>-0.607761</td>\n","      <td>-0.089610</td>\n","      <td>2.087741</td>\n","      <td>-1.045992</td>\n","      <td>-0.478951</td>\n","      <td>-0.434330</td>\n","      <td>-0.612300</td>\n","      <td>-0.664321</td>\n","      <td>-0.607700</td>\n","      <td>-0.701963</td>\n","      <td>0.515561</td>\n","      <td>-0.147157</td>\n","      <td>0.698382</td>\n","      <td>-0.659444</td>\n","      <td>-0.863913</td>\n","      <td>0.324892</td>\n","      <td>1.048591</td>\n","      <td>-0.577468</td>\n","      <td>-2.198485</td>\n","      <td>-0.001076</td>\n","      <td>-0.287394</td>\n","      <td>-1.249913</td>\n","      <td>-0.026577</td>\n","      <td>-1.759232</td>\n","      <td>1.005570</td>\n","      <td>-0.025031</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>127</th>\n","      <td>0.428486</td>\n","      <td>-0.487526</td>\n","      <td>0.054567</td>\n","      <td>1.039311</td>\n","      <td>-0.107569</td>\n","      <td>0.995310</td>\n","      <td>-0.866271</td>\n","      <td>-0.264390</td>\n","      <td>-0.520171</td>\n","      <td>-0.083964</td>\n","      <td>-0.642408</td>\n","      <td>0.580857</td>\n","      <td>0.078143</td>\n","      <td>-0.069868</td>\n","      <td>-0.016183</td>\n","      <td>0.128474</td>\n","      <td>0.425907</td>\n","      <td>1.185161</td>\n","      <td>-0.432118</td>\n","      <td>-0.914758</td>\n","      <td>-1.712033</td>\n","      <td>-0.617397</td>\n","      <td>-0.032528</td>\n","      <td>-0.850749</td>\n","      <td>2.190445</td>\n","      <td>0.323139</td>\n","      <td>0.194245</td>\n","      <td>-0.337960</td>\n","      <td>-0.434829</td>\n","      <td>-0.288811</td>\n","      <td>2.549508</td>\n","      <td>1.532231</td>\n","      <td>-0.317523</td>\n","      <td>0.094197</td>\n","      <td>1.513003</td>\n","      <td>0.295946</td>\n","      <td>1.352816</td>\n","      <td>-0.59592</td>\n","      <td>0.042070</td>\n","      <td>-0.455978</td>\n","      <td>-0.487712</td>\n","      <td>2.210062</td>\n","      <td>-0.294809</td>\n","      <td>-1.176726</td>\n","      <td>-0.139798</td>\n","      <td>-0.291518</td>\n","      <td>0.928196</td>\n","      <td>1.470750</td>\n","      <td>1.067178</td>\n","      <td>-0.125593</td>\n","      <td>0.810348</td>\n","      <td>-0.735412</td>\n","      <td>-0.155477</td>\n","      <td>0.258475</td>\n","      <td>0.971950</td>\n","      <td>1.167305</td>\n","      <td>-0.587212</td>\n","      <td>-0.306915</td>\n","      <td>0.923603</td>\n","      <td>1.001386</td>\n","      <td>3.618550</td>\n","      <td>1.023261</td>\n","      <td>-1.180515</td>\n","      <td>1.851935</td>\n","      <td>0.575287</td>\n","      <td>-0.108472</td>\n","      <td>0.709316</td>\n","      <td>0.084093</td>\n","      <td>-0.458182</td>\n","      <td>0.861392</td>\n","      <td>-0.004202</td>\n","      <td>0.044291</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>128</th>\n","      <td>0.428486</td>\n","      <td>0.455409</td>\n","      <td>-0.335159</td>\n","      <td>-1.737370</td>\n","      <td>-0.248450</td>\n","      <td>4.060529</td>\n","      <td>-0.462012</td>\n","      <td>-0.529282</td>\n","      <td>-0.483808</td>\n","      <td>-0.215408</td>\n","      <td>0.372541</td>\n","      <td>0.313042</td>\n","      <td>0.395510</td>\n","      <td>0.432978</td>\n","      <td>-1.218266</td>\n","      <td>2.081909</td>\n","      <td>-0.708069</td>\n","      <td>0.008244</td>\n","      <td>1.492259</td>\n","      <td>0.890740</td>\n","      <td>-0.412239</td>\n","      <td>-0.810195</td>\n","      <td>-0.553900</td>\n","      <td>-2.002912</td>\n","      <td>-0.411668</td>\n","      <td>-0.093798</td>\n","      <td>-0.559329</td>\n","      <td>0.026964</td>\n","      <td>-1.253806</td>\n","      <td>-0.656419</td>\n","      <td>-1.417917</td>\n","      <td>1.300827</td>\n","      <td>0.016244</td>\n","      <td>0.910927</td>\n","      <td>0.215498</td>\n","      <td>-0.031668</td>\n","      <td>0.230930</td>\n","      <td>-0.59592</td>\n","      <td>-0.459628</td>\n","      <td>0.125336</td>\n","      <td>0.807083</td>\n","      <td>-0.583462</td>\n","      <td>1.102817</td>\n","      <td>0.016795</td>\n","      <td>-0.360054</td>\n","      <td>-0.160010</td>\n","      <td>0.041084</td>\n","      <td>-0.796165</td>\n","      <td>-0.498733</td>\n","      <td>-0.284486</td>\n","      <td>-1.546283</td>\n","      <td>0.607039</td>\n","      <td>-0.194512</td>\n","      <td>-0.898527</td>\n","      <td>0.996984</td>\n","      <td>0.005033</td>\n","      <td>-0.606654</td>\n","      <td>0.298130</td>\n","      <td>0.952313</td>\n","      <td>-0.489895</td>\n","      <td>-0.775616</td>\n","      <td>-0.285633</td>\n","      <td>-0.595441</td>\n","      <td>-0.856618</td>\n","      <td>-0.150296</td>\n","      <td>0.814523</td>\n","      <td>0.299500</td>\n","      <td>-0.168200</td>\n","      <td>-0.389223</td>\n","      <td>-1.141655</td>\n","      <td>0.676066</td>\n","      <td>0.113348</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>129 rows × 73 columns</p>\n","</div>"],"text/plain":["            0         1         2  ...        70        71  Emotions\n","0   -0.347844  2.189228  0.463538  ... -1.241912 -0.154762         0\n","1   -0.503110  0.280254 -1.347963  ... -0.806115 -0.198937         0\n","2   -0.037312  0.484960  0.963926  ... -0.336069 -0.500178         0\n","3   -0.192578 -0.145812  2.744152  ...  0.667798  0.917145         1\n","4   -0.347844 -1.565319 -0.585353  ... -1.628106  0.197839         0\n","..        ...       ...       ...  ...       ...       ...       ...\n","124 -0.037312  0.740708  0.579012  ...  0.579222 -1.008988         1\n","125 -0.968907 -1.075315 -0.279827  ... -0.887606 -0.295270         1\n","126 -0.037312 -1.216084  0.600663  ...  1.005570 -0.025031         1\n","127  0.428486 -0.487526  0.054567  ... -0.004202  0.044291         0\n","128  0.428486  0.455409 -0.335159  ...  0.676066  0.113348         0\n","\n","[129 rows x 73 columns]"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"yjObZYYKamOc"},"source":["# lst = []\n","# lst = [index for index,value in enumerate(pca.explained_variance_) if value > 1]\n","# data,pca = PComp(data, len(lst))\n","# data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IhD3x6vZQoiZ"},"source":["# Training and Testing Function"]},{"cell_type":"code","metadata":{"id":"69d9rEQiwqVC","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ca302b2c-22c9-4bbd-dd2a-15845189de4d"},"source":["####################################################################################\n","####################################################################################\n","####################################################################################\n","start_time=time.time()\n","print(method)\n","print(nn)\n","print(mm)\n","print(fileN)\n","LR_max_iter=500\n","\n","#Eps=0.1; prop=0.2; **w= 23 j= 1 average accuracy= 0.6640625\n","Eps=0.2 #por la normalización\n","prop=0.2\n","\n","CV=5\n","k=1\n","acc_max=0\n","for w in range(200,400):\n","   for j in range(200,400):\n","     path = pat\n","     file= path + fileN\n","\n","     data = pd.read_csv(file, header = None)\n","     data_name = []\n","     for i in range(len(data.axes[1])):\n","      if i== len(data.axes[1])-1:\n","        data_name = data_name + ['Emotions']\n","      else:\n","        data_name = data_name + [str(i)]\n","\n","     data_n = data_name\n","     data.columns = data_name\n","     data=normalize(data)\n","    #  data = LLEcomp(data, nn,mm) \n","    #  data,_ = PComp(data, 20)\n","    \n","     nf = data.shape[1]-1 # número de features (componentes)\n","     nc = nf # número de componetes (features) a utilizar\n","     col = list(range(nc))\n","    #  print(nf)\n","\n","     random.seed(w)  # 150\n","     rng=np.random.RandomState(j) # 155, 555\n","\n","     acc_acu=0\n","     accuracy_acc=0\n","     N=int(len(data)-k)\n","     step=1\n","     i=0\n","     for n in range(0,N,step):\n","       i+=1\n","       wd=data\n","       #print('i',i)\n","       #print('shuffle in',data)\n","       ar=suffle_df(wd, 0, n, k)\n","       #print('shuffle out',data)\n","       X_train, X_test, y_train, y_test = train_test_sets(ar, 0, k, Eps, prop,nn,mm)\n","      #  clf1 = SVC(kernel='rbf')\n","      #  clf2 = SVC(kernel='poly')\n","      #  clf3 = GaussianNB()\n","      #  clf4 = RandomForestClassifier()\n","      #  model = VotingClassifier(estimators = [('SVCrbf', clf1), ('SVCpoly', clf2), ('gnb', clf3), ('rf', clf4)], voting ='hard')\n","       #X_train, X_test, y_train, y_test = train_test_sets(ar, 0, k, prop)\n","       #print('X_train=', X_train.shape[0])\n","       # --------------------------------------------------------------\n","       ##model = DecisionTreeClassifier(max_depth = 15)\n","      #  model = RandomForestClassifier()\n","       ##model = LinearSVC(C=2, tol = 1e-5, max_iter = 500000)\n","      #  model = SVC(kernel='poly') # random.seed(50), pr=0.2, rand in augmented data=117 : 0.887\n","      #  model = GaussianNB() # norm01: 0.873239, 0.859, 0.8450, 0.8169\n","\n","       ##model = MultinomialNB(class_prior=[0.55, 0.45])  # class_prior can be used to deal with class inbalance\n","       ##model = LogisticRegression(C=5, tol = 1e-5, max_iter = 1000000)\n","       ##model = LogisticRegressionCV(cv=CV, solver='liblinear', scoring='accuracy', random_state=random.randint(1,1000), n_jobs=-1, verbose=0, max_iter=LR_max_iter) # norm01: 0.85915 \n","       ##model = LogisticRegressionCV(cv=CV, scoring='accuracy', random_state=random.randint(1,1000), n_jobs=-1, verbose=0, max_iter=LR_max_iter) # norm01 87.3\n","       model = SVC(kernel='rbf') # random.seed(50), pr=0.2, rand in augmented data=117 : 0.887\n","\n","       #Fitting de Random Forest\n","       model.fit(X_train, y_train)\n","\n","       acc = model.score(X_test, y_test)\n","       acc_acu += acc\n","\n","       predictions = model.predict(X_test)\n","       cm = confusion_matrix(y_test, predictions)\n","       accuracy= metrics.accuracy_score(y_test, predictions)\n","       accuracy_acc += accuracy\n","     #print('w=',w,'j=',j,\"average accuracy=\",acc_acu/i)\n","\n","     if ((w+j)%100==0):\n","       print('w=',w,'j=',j,\"average accuracy=\",acc_acu/i)\n","       print('Execution time: ', time.time()-start_time)\n","\n","\n","     acc_ave=acc_acu/i\n","     if acc_ave > acc_max:\n","       acc_max=acc_ave\n","       print('****** w=',w,'j=',j,\"average accuracy=\",acc_acu/i,'************')\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["OTH\n","60\n","5\n","dep_writing_Total_25_oth0.15_ith0.5.csv\n","w= 200 j= 200 average accuracy= 0.96875\n","Execution time:  11.650394201278687\n","****** w= 200 j= 200 average accuracy= 0.96875 ************\n","****** w= 200 j= 203 average accuracy= 0.9765625 ************\n","w= 200 j= 300 average accuracy= 0.96875\n","Execution time:  1127.696886062622\n","****** w= 200 j= 353 average accuracy= 0.984375 ************\n","w= 201 j= 299 average accuracy= 0.9609375\n","Execution time:  3356.2060146331787\n","w= 201 j= 399 average accuracy= 0.9765625\n","Execution time:  4475.653032064438\n","w= 202 j= 298 average accuracy= 0.9609375\n","Execution time:  5586.819779634476\n","w= 202 j= 398 average accuracy= 0.96875\n","Execution time:  6716.065522909164\n","w= 203 j= 297 average accuracy= 0.9765625\n","Execution time:  7829.258897781372\n","w= 203 j= 397 average accuracy= 0.96875\n","Execution time:  8958.554121732712\n","w= 204 j= 296 average accuracy= 0.9609375\n","Execution time:  10083.043713569641\n","w= 204 j= 396 average accuracy= 0.96875\n","Execution time:  11208.63200044632\n","w= 205 j= 295 average accuracy= 0.96875\n","Execution time:  12325.963368654251\n","w= 205 j= 395 average accuracy= 0.96875\n","Execution time:  13456.317914009094\n","w= 206 j= 294 average accuracy= 0.96875\n","Execution time:  14578.91608595848\n","w= 206 j= 394 average accuracy= 0.9609375\n","Execution time:  15714.696544408798\n","w= 207 j= 293 average accuracy= 0.9609375\n","Execution time:  16835.77958893776\n","w= 207 j= 393 average accuracy= 0.96875\n","Execution time:  17966.450501441956\n","w= 208 j= 292 average accuracy= 0.96875\n","Execution time:  19091.71137690544\n","w= 208 j= 392 average accuracy= 0.96875\n","Execution time:  20227.376825094223\n","w= 209 j= 291 average accuracy= 0.9609375\n","Execution time:  21346.75027513504\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ndklXJX6DF4z"},"source":[" "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NZXWmhYbgIOW"},"source":[""],"execution_count":null,"outputs":[]}]}